{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to train word2vec (SkipGramModel) with negative sampling. We will use huggingface tokenizer and datasets, just to \n",
    "make data processing simpler. \n",
    "\n",
    "Almost copied from https://github.com/Andras7/word2vec-pytorch/blob/master/word2vec/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import init\n",
    "\n",
    "# huggingface\n",
    "# import datasets\n",
    "# from transformers import AutoTokenizer\n",
    "# from tokenizers import \n",
    "\n",
    "# from fastai.text.data import Numericalize\n",
    "import pytorch_lightning as pl\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataloader from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
    "# !tar -zxvf wikitext-2.tgz\n",
    "# !head wikitext-2/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "    \n",
    "    def update_vocab(self, tokens: List):\n",
    "        for token in tokens:\n",
    "            if not token in self.word2idx.keys():\n",
    "                next_idx = len(self.idx2word)\n",
    "                self.word2idx[token] = next_idx\n",
    "                self.idx2word[next_idx] = token\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataSet(Dataset):\n",
    "    def __init__(self, filepath, n_half=2, neg_size=5):\n",
    "        self.n_half = n_half\n",
    "        self.neg_size = neg_size\n",
    "        \n",
    "        # read the text line by line\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        with open(filepath,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f'num_lines is {len(lines)}')\n",
    "        \n",
    "        # tokenize and combine the text\n",
    "        self.tokenized_text = []\n",
    "        for line in lines:\n",
    "            self.tokenized_text.extend(tokenizer.tokenize(line.lower()))\n",
    "            \n",
    "        # build vocab\n",
    "        self.vocab = Vocab()\n",
    "        self.vocab.update_vocab(self.tokenized_text)\n",
    "            \n",
    "        self.len = len(self.tokenized_text) - 2*self.n_half\n",
    "        \n",
    "    def __getitem__(self, center_idx):\n",
    "        '''\n",
    "        return the index of context words and center word\n",
    "        '''\n",
    "        idx = random.randrange(self.n_half, self.len - self.n_half)\n",
    "        item_txt = (self.tokenized_text[center_idx:center_idx+self.n_half] + \n",
    "                    self.tokenized_text[center_idx+self.n_half+1:center_idx+self.n_half*2+1])\n",
    "        context_idxs = [self.vocab.word2idx[word] for word in item_txt]\n",
    "        center_idx = self.vocab.word2idx[self.tokenized_text[center_idx+self.n_half]]\n",
    "        neg_idxs = random.choices(list(self.vocab.idx2word.keys()), k=self.neg_size)\n",
    "        return torch.tensor(center_idx), torch.tensor(context_idxs), torch.tensor(neg_idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, emb_size, hparams):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        # center word embeddings\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        # context word embeddings\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.loss_func = nn.NLLLoss()\n",
    "        \n",
    "        # initialization\n",
    "        initrange = 1.0/emb_size\n",
    "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
    "        init.constant_(self.v_embeddings.weight.data, 0)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return train_dl\n",
    "        \n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        # similarity score with positive samples\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_u = self.u_embeddings(pos_u.unsqueeze(1).repeat(1,pos_v.shape[1]))\n",
    "        pos_similarity = torch.sum(torch.mul(emb_u, emb_v), dim=[1,2])\n",
    "        pos_score = -F.logsigmoid(pos_similarity)\n",
    "\n",
    "        # similarity score with negative samples\n",
    "        emb_neg_v = v_embeddings(neg_v)\n",
    "        emb_neg_u = v_embeddings(pos_u.unsqueeze(1).repeat(1,neg_v.shape[1]))\n",
    "        neg_similarity = torch.sum(torch.mul(emb_neg_v, emb_neg_u), dim=[1,2])\n",
    "        neg_score = -F.logsigmoid(-neg_similarity)\n",
    "        \n",
    "        return torch.mean(pos_score + neg_score)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pos_u, pos_v, neg_v = batch\n",
    "        score = self.forward(pos_u, pos_v, neg_v)\n",
    "        return pl.TrainResult(minimize=score)  \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(bsz=512, lr=0.0001, max_epochs=10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--bsz', type=int)\n",
    "parser.add_argument('--max-epochs', type=int)\n",
    "parser.add_argument('--lr', type=float)\n",
    "hparams = parser.parse_args('--bsz 512 --max-epochs 10 --lr 0.0001'.split())\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines is 37333\n"
     ]
    }
   ],
   "source": [
    "filepath = 'wikitext-2/train.csv'\n",
    "ds = NGramDataSet(filepath)\n",
    "train_dl = DataLoader(ds, batch_size=hparams.bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | u_embeddings | Embedding | 8 M   \n",
      "1 | v_embeddings | Embedding | 8 M   \n",
      "2 | loss_func    | NLLLoss   | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5fe48649404c5b8a7ff575b433bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(ds.vocab.word2idx)\n",
    "emb_size = 300\n",
    "\n",
    "model = SkipGramModel(vocab_size, emb_size, hparams)\n",
    "trainer = pl.Trainer(max_epochs=hparams.max_epochs)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
