{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "We will use a bert model to classify text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scripts.kaggle.bert_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d rmisra/news-category-dataset\n",
    "# !unzip news-category-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "lines = []\n",
    "filepath = 'News_Category_Dataset_v2.json'\n",
    "with open(filepath) as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append(json.loads(line))\n",
    "df = pd.DataFrame(lines)\n",
    "df['text'] = df['headline'] + ' ' + df['short_description']\n",
    "del lines\n",
    "\n",
    "# filter lines without headline\n",
    "df['text_len'] = df.text.str.len()\n",
    "df = df[df.text_len > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>headline_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description        date  headline_len  \n",
       "0  She left her husband. He killed their children...  2018-05-26            64  \n",
       "1                           Of course it has a song.  2018-05-26            75  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape - (190805, 8) val.shape - (10042, 8)\n",
      "max_len - 1492\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "# split into train and validation\n",
    "num_val = int(df.shape[0] * 0.05)\n",
    "val_mask = np.random.choice(np.arange(df.shape[0]),num_val, replace=False)\n",
    "df_train = df[~df.index.isin(val_mask)].copy()\n",
    "df_val = df[df.index.isin(val_mask)].copy()\n",
    "print(f'train.shape - {df_train.shape} val.shape - {df_val.shape}')\n",
    "max_len = df['text'].str.len().max() + 5\n",
    "print(f'max_len - {max_len}')\n",
    "\n",
    "max_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 10, 10, ..., 28, 28, 28])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, df, label_encoder, max_len):\n",
    "        self.df = df\n",
    "        self.le = label_encoder\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_text = self.tokenizer.tokenize(self.df.at[idx,'text'])\n",
    "        \n",
    "        # https://huggingface.co/transformers/glossary.html\n",
    "        sequence_dict = self.tokenizer.encode_plus(tokenized_text, max_length=max_len, pad_to_max_length=True)\n",
    "        cat_tensor = torch.tensor(self.le.transform([self.df.at[idx,'category']]))[0]\n",
    "        return torch.tensor(sequence_dict['input_ids']), torch.tensor(sequence_dict['attention_mask']), cat_tensor\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [nn.BatchNorm1d(n_out if lin_first else n_in)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin + layers if lin_first else layers + lin\n",
    "        super().__init__(*layers)\n",
    "\n",
    "class BertFineTuner(LightningModule):\n",
    "    def __init__(self, hparams, *args, **kwargs):\n",
    "        self.hparams = hparams\n",
    "#         print('num_classes: ', hparams.num_cls)\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\n",
    "        self.lin = LinBnDrop(self.bert.config.hidden_size, hparams.num_cls)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # freeze all bert parameters\n",
    "        for param in self.named_parameters():\n",
    "            if param[0].startswith('bert'):\n",
    "                param[1].requires_grad = False\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--bsz', default=8, type=int, help='batch_size', )\n",
    "        parser.add_argument('--val-bsz', default=8, type=int, help='batch_size', )\n",
    "        parser.add_argument('--lr', default=0.001, type=int, help='batch_size', )\n",
    "        return parser\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        h, _, attn = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        h_cls = h[:,0]\n",
    "        logits = self.lin(h_cls)\n",
    "        return logits, attn\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(TextDS(df_train.reset_index(), le, max_len), batch_size=self.hparams.bsz, num_workers=8)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(TextDS(df_val.reset_index(), le, max_len),batch_size=self.hparams.val_bsz, num_workers=8)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, cat_idx = batch\n",
    "        logits, attn = self.forward(input_ids, attention_mask)\n",
    "        loss = self.loss_func(logits, cat_idx)\n",
    "#       \n",
    "        if batch_idx%10 == 0:\n",
    "            self.logger.log_metrics({'loss': loss},step=self.current_epoch)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_ids):\n",
    "        input_ids, attention_mask, cat_idx = batch\n",
    "        logits, attn = self.forward(input_ids, attention_mask)\n",
    "        loss = self.loss_func(logits, cat_idx)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=(self.hparams.lr))\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "parser = ArgumentParser()\n",
    "parser = BertFineTuner.add_model_specific_args(parser)\n",
    "parser.add_argument('--num-cls', type=int, default=len(le.classes_))\n",
    "parser.add_argument('--gpus', default=0)\n",
    "parser.add_argument('--lr-gpu', default=0)\n",
    "parser.add_argument('--val-check-interval', type=int, default=100)\n",
    "parser.add_argument('--max-epochs', type=int, default=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = parser.parse_args('--bsz 64'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "hparams = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "trainer = Trainer(gpus=hparams.lr_gpu)\n",
    "model = BertFineTuner(hparams, finding_lr=True)\n",
    "\n",
    "# find learning rate\n",
    "train_dl = DataLoader(TextDS(df_train.reset_index(), le, max_len), batch_size=8, num_workers=8)\n",
    "val_dl = DataLoader(TextDS(df_val.reset_index(), le, max_len), batch_size=8, num_workers=8)\n",
    "lr_finder = trainer.lr_find(model, train_dataloader=train_dl,val_dataloaders=[val_dl] )\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "new_lr = lr_finder.suggestion()\n",
    "print('new lr: ', new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/achinta/bert-text-cls\" target=\"_blank\">https://app.wandb.ai/achinta/bert-text-cls</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/achinta/bert-text-cls/runs/20tm5t8e\" target=\"_blank\">https://app.wandb.ai/achinta/bert-text-cls/runs/20tm5t8e</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | bert      | BertModel        | 108 M \n",
      "1 | lin       | LinBnDrop        | 33 K  \n",
      "2 | loss_func | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52efcae1ea354b6da665bde8ffd426fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "wandb_logger = WandbLogger(name='achinta',project='bert-text-cls')\n",
    "hparams.lr = new_lr\n",
    "model = BertFineTuner(hparams)\n",
    "\n",
    "trainer = Trainer(gpus=hparams.gpus,max_epochs=hparams.max_epochs,\n",
    "                  logger=wandb_logger, val_check_interval=hparams.val_check_interval)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 104-bert-text-classification.ipynb.\n",
      "/etc/profile.d/lang.sh: line 19: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory\n",
      "bert_classification.py                        100% 6558    10.2KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib --fname 104-bert-text-classification.ipynb\n",
    "!scp ../ml/scripts/kaggle/bert_classification.py rc:/n/home11/tatacomm/kaggle/datasets/news-category-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.94203547974329"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headline.str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
